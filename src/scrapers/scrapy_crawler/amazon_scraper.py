import os
import sys
import time
import json
import random
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

# Add src to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from src.data.database import Database
from src.utils.logger import setup_logger
from .amazon_spider import AmazonProductSpider, SCRAPED_ITEMS, RotateUserAgentMiddleware, AmazonAntiBlockMiddleware


class AmazonScrapyRunner:
    """Runner class for Amazon Scrapy scraper with database integration"""
    
    def __init__(self, db_path=None):
        if db_path is None:
            # Calculate path to project root for database
            project_root = os.path.join(os.path.dirname(__file__), '..', '..', '..')
            db_path = os.path.join(project_root, 'scraped_data.db')
        self.db = Database(db_path)
        self.logger = setup_logger(__name__, log_file='../logs/amazon_scraper.log')
    
    def run_scraper(self, search_terms, max_pages=1):
        """Run Amazon scraper with adaptive anti-detection strategies"""
        self.logger.info(f"üöÄ Starting Amazon scraper for terms: {search_terms}")
        
        try:
            # Create job for tracking
            job_id = self.db.queue_job(', '.join(search_terms))
            self.logger.info(f"üìù Created job ID: {job_id}")
            
            # Clear any existing temp file
            temp_file = 'temp_scraped_items.json'
            try:
                os.remove(temp_file)
            except FileNotFoundError:
                pass
            
            # Enhanced crawler settings with adaptive delays
            settings = get_project_settings()
            settings.update({
                'DOWNLOAD_DELAY': random.uniform(5, 10),  # Even longer delays
                'RANDOMIZE_DOWNLOAD_DELAY': True,
                'CONCURRENT_REQUESTS': 1,
                'RETRY_TIMES': 1,  # Minimal retries
                'AUTOTHROTTLE_ENABLED': True,
                'AUTOTHROTTLE_START_DELAY': 8,
                'AUTOTHROTTLE_MAX_DELAY': 20,
                'AUTOTHROTTLE_TARGET_CONCURRENCY': 0.3,  # Very conservative
                'COOKIES_ENABLED': True,
                'ROBOTSTXT_OBEY': False,
            })
            
            # Start crawler with enhanced settings
            process = CrawlerProcess(settings)
            
            self.logger.info(f"üï∏Ô∏è Starting Scrapy crawler process...")
            
            process.crawl(
                AmazonProductSpider, 
                search_terms=search_terms, 
                max_pages=max_pages,
                job_id=job_id
            )
            process.start()
            
            # Read results from the temp file created by CollectorPipeline
            scraped_items = self._read_scraped_results(temp_file)
            
            # Process results
            if scraped_items:
                self.logger.info(f"üéâ Collected {len(scraped_items)} Amazon products!")
                
                # Convert items to the format expected by database
                products_for_db = []
                for item in scraped_items:
                    products_for_db.append({
                        'name': item.get('name', 'Unknown'),
                        'price': item.get('price'),
                        'link': item.get('link', ''),
                        'image': '',  # Amazon scraper doesn't extract images yet
                        'availability': item.get('availability', 'In Stock'),
                        'scrape_time': item.get('scrape_time'),
                        'search_term': item.get('search_term', search_terms[0] if search_terms else 'unknown'),
                        'source': item.get('source', 'Amazon')
                    })
                
                # Save to database
                self.db.insert_products(products_for_db, job_id=job_id)
                self.db.mark_job_complete(job_id)
                
                # Clean up temp file
                try:
                    os.remove(temp_file)
                except FileNotFoundError:
                    pass
                
                self.logger.info(f"‚úÖ Successfully saved {len(products_for_db)} Amazon products to database!")
                return scraped_items
            else:
                self.logger.warning(f"‚ö†Ô∏è No items collected from Amazon")
                self._handle_blocking_scenario()
                return []
                
        except Exception as e:
            self.logger.error(f"‚ùå Error running Amazon scraper: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            self._handle_blocking_scenario()
            return []
    
    def _read_scraped_results(self, temp_file):
        """Read scraped results from the temp JSON file created by CollectorPipeline"""
        try:
            if os.path.exists(temp_file):
                with open(temp_file, 'r', encoding='utf-8') as f:
                    items = json.load(f)
                self.logger.info(f"üìñ Read {len(items)} items from temp file")
                return items
            else:
                self.logger.warning(f"‚ö†Ô∏è Temp file {temp_file} not found")
                return []
        except (json.JSONDecodeError, Exception) as e:
            self.logger.error(f"‚ùå Error reading temp file: {e}")
            return []
    
    def _handle_blocking_scenario(self):
        """Handle the case when Amazon is blocking us"""
        self.logger.info("\n" + "="*50)
        self.logger.info("‚ö†Ô∏è Amazon Anti-Bot Protection Active")
        self.logger.info("üõ°Ô∏è Amazon is blocking automated access (this is normal)")
        self.logger.info("üìã Your scraping framework is working correctly!")
        self.logger.info("")
        self.logger.info("üí° Alternative options:")
        self.logger.info("   ‚Ä¢ Use eBay scraper (more reliable for demos)")
        self.logger.info("   ‚Ä¢ Use Static scraper (BooksToScrape - 100% reliable)")
        self.logger.info("   ‚Ä¢ For Amazon, would need enterprise-level anti-detection")
        self.logger.info("")
        self.logger.info("‚úÖ Try the eBay or Static scrapers to see your framework in action!")
        self.logger.info("="*50)


def run_amazon_scraper():
    """Standalone function to run Amazon scraper"""
    runner = AmazonScrapyRunner()
    search_terms = ['laptop']
    results = runner.run_scraper(search_terms, max_pages=1)
    return results


if __name__ == "__main__":
    run_amazon_scraper() 